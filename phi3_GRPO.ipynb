{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SreeyaSrikanth/RL-Prompt-Compression/blob/main/phi3_GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz66XejRg-fH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWzNqHdCg-fI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "except: is_t4 = False\n",
        "get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "!uv pip install -qqq --upgrade \\\n",
        "    unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
        "!uv pip install -qqq {get_triton}\n",
        "!uv pip install transformers==4.56.2\n",
        "!uv pip install --no-deps trl==0.22.2\n",
        "!uv pip install -qqq sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8wkkXFOW9CR",
        "outputId": "34bcc06a-a6de-4966-cdc2-7aab8e30a31f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.7.0+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-27 15:31:51 [__init__.py:244] Automatically detected platform cuda.\n",
            "ERROR 10-27 15:32:02 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, TextStreamer\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import diskcache\n",
        "import re\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Ax_D59YDEP",
        "outputId": "192b23bf-028a-47ff-f034-bf2ebe4fa7bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device not set, defaulting to: cuda\n"
          ]
        }
      ],
      "source": [
        "if 'device' not in locals():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Device not set, defaulting to: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rezoBIE1aXRA"
      },
      "source": [
        "Load up `Phi-3 4k Instruct`, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkIvEkIIkEyB",
        "outputId": "95e10f0e-24a2-4abd-c147-39d6bf4b832b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.10.10: Fast Mistral patching. Transformers: 4.56.2. vLLM: 0.9.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Model and Tokenizer Loaded (Phi-3 Mini 4-bit).\n"
          ]
        }
      ],
      "source": [
        "## CELL: Load Model and Tokenizer (Phi-3 Mini 4-bit)\n",
        "\n",
        "from unsloth import FastLanguageModel # Use FastLanguageModel for Phi-3\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048 # Match Fine_Tuning.ipynb\n",
        "dtype = None # Auto detection for Phi-3\n",
        "\n",
        "# Load Phi-3 Mini 4-bit model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\", # Switched back to Phi-3\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = True,  # Enable 4-bit quantization like Fine_Tuning.ipynb\n",
        "    # token = \"hf_...\", # Add your Hugging Face token if needed\n",
        ")\n",
        "print(\"Model and Tokenizer Loaded (Phi-3 Mini 4-bit).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nHQVsQJ2roh"
      },
      "source": [
        "Add LoRA adapters so we only need to update a small amount of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CLfym_T0lQ-",
        "outputId": "661b56df-509e-409b-8dfe-50a2b700b87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.10.10 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA Adapters Added (Matching Fine_Tuning.ipynb Rank/Alpha/Modules).\n",
            "trainable params: 119,537,664 || all params: 3,940,617,216 || trainable%: 3.0335\n"
          ]
        }
      ],
      "source": [
        "## CELL: Add LoRA Adapters (Matching Fine_Tuning.ipynb)\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64,          # Matched\n",
        "    lora_alpha = 128,# Matched\n",
        "    target_modules = [ # Explicitly list modules like in Fine_Tuning.ipynb\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_dropout = 0,   # Matched\n",
        "    bias = \"none\",      # Matched\n",
        "    use_gradient_checkpointing = \"unsloth\", # Use Unsloth's optimized version\n",
        "    random_state = 3407, # Matched\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "print(\"LoRA Adapters Added (Matching Fine_Tuning.ipynb Rank/Alpha/Modules).\")\n",
        "model.print_trainable_parameters() # Optional: See trainable parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-L4bV_1cmf4"
      },
      "source": [
        "Prepare the GSM8K dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEibULDtlOMU",
        "outputId": "41af355d-3874-4c4d-c0c2-6c77c5ada7ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer'],\n",
              "    num_rows: 7473\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dataset = load_dataset(\"openai/gsm8k\", \"main\", split = \"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.select(range(len(dataset) // 3))\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQKroKogT12y",
        "outputId": "d34a1a15-e553-4263-be7e-15574c8d9011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer'],\n",
              "    num_rows: 2491\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8Tx6ZUGqLyu",
        "outputId": "73c39df6-b774-424d-f0ef-dbaf602282b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New dataset example:\n",
            "Original Prompt: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
            "Original Output: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
            "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
            "#### 72\n"
          ]
        }
      ],
      "source": [
        "print(\"New dataset example:\")\n",
        "print(f\"Original Prompt: {dataset[0]['question']}\")\n",
        "print(f\"Original Output: {dataset[0]['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFXmqsL1OetD",
        "outputId": "6e2486ca-f5b7-4bdc-e4ab-42cb5eba766f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72.0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "def extract_hash_answer(text):\n",
        "    parts = text.split(\"####\")\n",
        "    if len(parts) > 1:\n",
        "        text_to_search = parts[-1].strip()\n",
        "    else:\n",
        "        text_to_search = text\n",
        "    numbers = re.findall(r'[-+]?\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\d+\\.\\d+|\\.\\d+|\\d+', text_to_search)\n",
        "    if numbers:\n",
        "        try: return float(numbers[-1].replace(',', ''))\n",
        "        except ValueError: return None\n",
        "    return None\n",
        "extract_hash_answer(dataset[0][\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6uoYlChoRMMy",
        "outputId": "b5767931-9d2a-4063-870f-c38b899cec29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "def extract_reasoning(text):\n",
        "    parts = text.split(\"####\")\n",
        "    if len(parts) > 1:\n",
        "        return parts[0].strip()\n",
        "    else:\n",
        "        return text.strip() if text else \"\"\n",
        "extract_reasoning(dataset[0][\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHjiV3kGi8Y9",
        "outputId": "b383dcae-1c17-491b-99b7-082e8fecddea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an expert prompt compressor. Your task is to rewrite the given prompt to be as short as possible while ensuring that a large language model can still generate the same, high-quality response. Retain all key constraints and entities from the original prompt.\n"
          ]
        }
      ],
      "source": [
        "system_prompt = (\n",
        "    \"You are an expert prompt compressor. Your task is to rewrite the given prompt \"\n",
        "    \"to be as short as possible while ensuring that a large language model \"\n",
        "    \"can still generate the same, high-quality response. Retain all key constraints \"\n",
        "    \"and entities from the original prompt.\"\n",
        ")\n",
        "print(system_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "c8f89c484c414a53a017674039994f0b",
            "f490d1fac94445c88d4b3188f7f45f68",
            "0e5697aa37604c5194fcb2c63cf50679",
            "eda54a9c55894dea9a658cdcbdd420e4",
            "ed7d987055b7403db8475d028d45d182",
            "43bcab877623412dad9d507ef89c73bb",
            "2def6f8fa9b6462ea7ba99ffed770ff3",
            "0ea37091e24143d3b88bd855ad931d78",
            "87a28938bf634986ad31a666111ca757",
            "b32d051c6073485790e7a1a15fc7e23a",
            "913284b9fdc140ff80fe511d29bdea4a"
          ]
        },
        "id": "5tkTF5Hmlhl-",
        "outputId": "0ce043fa-f783-4757-9dc8-c5d6130a052b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2491 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8f89c484c414a53a017674039994f0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
              " 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72',\n",
              " 'prompt': [{'content': 'You are an expert prompt compressor. Your task is to rewrite the given prompt to be as short as possible while ensuring that a large language model can still generate the same, high-quality response. Retain all key constraints and entities from the original prompt.',\n",
              "   'role': 'system'},\n",
              "  {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
              "   'role': 'user'}],\n",
              " 'original_prompt': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
              " 'original_output': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72',\n",
              " 'ground_truth_answer': 72.0}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "dataset = dataset.map(lambda x: {\n",
        "    \"prompt\" : [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\",   \"content\": x[\"question\"]},\n",
        "    ],\n",
        "    \"original_prompt\": x[\"question\"],\n",
        "    \"original_output\": x[\"answer\"],\n",
        "    \"ground_truth_answer\": extract_hash_answer(x[\"answer\"]),\n",
        "})\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naReLWylyYKN"
      },
      "source": [
        "Create SFT Dataset and SFT Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "de3c926e20484accb0d6faf1dd595f62",
            "5695906539b74468b65d05a6fa3dfd0f",
            "202724a54bb84aa2882f370a07de7a26",
            "b9555846f727431db1394071f386f2de",
            "fa802c2cd3e644679ba5f3c7a612d06d",
            "d559214231404abdaa5dda044d027040",
            "282f973d117f4e7991aaa35bafa63200",
            "fce34a05e01f470bb13a03cce007cda3",
            "944f7b67bdf84676bad404dadbdfc717",
            "8e3a4bbcd4ca400db08502efd212f743",
            "3f2cc85149924278bc4893a524897277"
          ]
        },
        "id": "iw3SIHBrrZYj",
        "outputId": "df5a4758-2db0-4966-b8d9-8f2506343dc6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de3c926e20484accb0d6faf1dd595f62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Example of formatted SFT data ---\n",
            "<|system|>\n",
            "You are an expert prompt compressor. Your task is to rewrite the given prompt to be as short as possible while ensuring that a large language model can still generate the same, high-quality response. Retain all key constraints and entities from the original prompt.<|end|>\n",
            "<|user|>\n",
            "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. In June, she sold 5 fewer clips than in May. How many clips did Natalia sell in total?<|end|>\n",
            "<|assistant|>\n",
            "Natalia sold 48 clips in April, half as many in May, and 5 fewer than May in June. Total clips sold?<|end|>\n",
            "<|endoftext|>\n",
            "\n",
            "Dataset Prepared and Formatted.\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Your provided dataset\n",
        "warmup_samples = [\n",
        "    {\"question\": \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. In June, she sold 5 fewer clips than in May. How many clips did Natalia sell in total?\", \"compressed_question\": \"Natalia sold 48 clips in April, half as many in May, and 5 fewer than May in June. Total clips sold?\"},\n",
        "    {\"question\": \"A bakery made 100 donuts in the morning. They sold 3/4 of them. In the afternoon, they made another 50 donuts. How many donuts did the bakery have at the end of the day?\", \"compressed_question\": \"A bakery had 100 donuts, sold 3/4, then made 50 more. How many donuts are left?\"},\n",
        "    {\"question\": \"John is reading a book that is 450 pages long. He read 1/3 of the book on Monday and 100 pages on Tuesday. How many pages does he have left to read?\", \"compressed_question\": \"A 450-page book: John read 1/3 on Monday and 100 pages on Tuesday. How many pages are left?\"},\n",
        "    {\"question\": \"A rectangular garden is 15 meters long and 8 meters wide. A fence is built around it. If the fencing material costs $12 per meter, what is the total cost of the fence?\", \"compressed_question\": \"A 15m by 8m rectangular garden needs a fence. Fencing costs $12/meter. What's the total cost?\"},\n",
        "    {\"question\": \"Sarah has a budget of $200 for shopping. She buys a pair of shoes for $75 and a dress for $50. She then finds a handbag that is 20% off its original price of $60. Can she afford the handbag?\", \"compressed_question\": \"Sarah's budget is $200. She buys $75 shoes and a $50 dress. Can she afford a $60 handbag with a 20% discount?\"},\n",
        "    {\"question\": \"A train travels at a speed of 80 kilometers per hour. It leaves station A at 9:00 AM and is scheduled to arrive at station B at 1:30 PM. How far is station B from station A?\", \"compressed_question\": \"A train travels at 80 km/h from 9:00 AM to 1:30 PM. What is the distance traveled?\"},\n",
        "    {\"question\": \"A recipe for a cake requires 250 grams of flour, 150 grams of sugar, and 100 grams of butter. If you want to make 3 cakes, how much of each ingredient do you need in total?\", \"compressed_question\": \"A cake needs 250g flour, 150g sugar, and 100g butter. How much of each ingredient for 3 cakes?\"},\n",
        "    {\"question\": \"Mark is saving for a new bike that costs $500. He already has $150 saved. If he saves $25 every week, how many weeks will it take him to save enough money for the bike?\", \"compressed_question\": \"A bike costs $500. Mark has $150 and saves $25 weekly. How many weeks until he can afford it?\"},\n",
        "    {\"question\": \"There are 30 students in a class. 2/5 of them are boys. On a particular day, 1/3 of the boys are absent. How many boys are present in the class on that day?\", \"compressed_question\": \"A class has 30 students, 2/5 are boys. If 1/3 of the boys are absent, how many are present?\"},\n",
        "    {\"question\": \"A water tank has a capacity of 5000 liters. It is currently 60% full. If water is being added to the tank at a rate of 100 liters per minute, how long will it take to fill the tank completely?\", \"compressed_question\": \"A 5000L tank is 60% full. If filled at 100L/min, how long until it's full?\"},\n",
        "    {\"question\": \"A bookstore is having a sale where all books are 15% off. If a book originally costs $20, what is the sale price? If you pay with a $50 bill, how much change do you get?\", \"compressed_question\": \"A $20 book is 15% off. What is the sale price and change from $50?\"},\n",
        "    {\"question\": \"A farmer has 120 chickens and cows in total. The number of chickens is three times the number of cows. How many chickens and how many cows does the farmer have?\", \"compressed_question\": \"A farmer has 120 chickens and cows. There are three times as many chickens as cows. How many of each?\"},\n",
        "    {\"question\": \"A car's fuel tank holds 50 liters of gasoline. The car consumes 8 liters of gasoline per 100 kilometers. If the tank is full, how far can the car travel before it runs out of fuel?\", \"compressed_question\": \"A car has a 50L fuel tank and consumes 8L/100km. What is the car's maximum range on a full tank?\"},\n",
        "    {\"question\": \"The sum of three consecutive integers is 147. What are the three integers?\", \"compressed_question\": \"The sum of three consecutive integers is 147. Find the integers.\"},\n",
        "    {\"question\": \"A company's profit was $500,000 in 2022. In 2023, the profit increased by 12%. What was the profit in 2023?\", \"compressed_question\": \"A company's profit was $500,000. It increased by 12% the next year. What was the new profit?\"},\n",
        "    {\"question\": \"A library has 2500 books. 40% are fiction, 30% are non-fiction, and the rest are reference books. How many reference books are there in the library?\", \"compressed_question\": \"A library with 2500 books has 40% fiction and 30% non-fiction. How many are reference books?\"},\n",
        "    {\"question\": \"A pizza is cut into 8 equal slices. Tom eats 3 slices, and Jane eats 2 slices. What fraction of the pizza is left?\", \"compressed_question\": \"A pizza has 8 slices. Tom eats 3 and Jane eats 2. What fraction remains?\"},\n",
        "    {\"question\": \"A swimming pool is 25 meters long, 10 meters wide, and 2 meters deep. What is the volume of the pool in cubic meters?\", \"compressed_question\": \"A swimming pool is 25m long, 10m wide, and 2m deep. Calculate its volume.\"},\n",
        "    {\"question\": \"An airplane flies at an altitude of 35,000 feet. A submarine is at a depth of 1,500 feet below sea level. What is the vertical distance between the airplane and the submarine?\", \"compressed_question\": \"What is the vertical distance between an airplane at 35,000 feet altitude and a submarine 1,500 feet deep?\"},\n",
        "    {\"question\": \"A factory produces 600 widgets per day. Due to a machine breakdown, the production is reduced by 25%. How many widgets are produced on that day?\", \"compressed_question\": \"A factory that produces 600 widgets per day has a 25% reduction in output. How many widgets are made?\"},\n",
        "    {\"question\": \"If a shirt costs $45 and is on sale for 30% off, how much does it cost after the discount?\", \"compressed_question\": \"What is the price of a $45 shirt after a 30% discount?\"},\n",
        "    {\"question\": \"A movie starts at 6:45 PM and lasts for 2 hours and 20 minutes. What time does the movie end?\", \"compressed_question\": \"A movie starts at 6:45 PM and runs for 2 hours 20 minutes. When does it end?\"},\n",
        "    {\"question\": \"A garden has 12 rows of tomato plants. Each row has 8 plants. If each plant produces 5 tomatoes, what is the total number of tomatoes produced?\", \"compressed_question\": \"A garden has 12 rows with 8 tomato plants each. Each plant yields 5 tomatoes. What is the total tomato yield?\"},\n",
        "    {\"question\": \"David weighs 80 kg. He goes on a diet and loses 15% of his weight. What is his new weight?\", \"compressed_question\": \"David, who weighs 80 kg, loses 15% of his weight. What is his new weight?\"},\n",
        "    {\"question\": \"A car rental company charges $50 per day plus $0.20 per mile. If you rent a car for 3 days and drive 200 miles, what is the total rental cost?\", \"compressed_question\": \"Car rental costs $50/day plus $0.20/mile. What's the total cost for 3 days and 200 miles?\"}\n",
        "]\n",
        "\n",
        "# System prompt needs to be defined (assuming it was in a previous cell)\n",
        "# Example:\n",
        "# system_prompt = (\n",
        "#     \"You are an expert prompt compressor...\"\n",
        "# )\n",
        "\n",
        "# Convert list to Hugging Face Dataset\n",
        "sft_dataset = Dataset.from_list(warmup_samples)\n",
        "\n",
        "# Function to format data into chat template\n",
        "def create_chat_prompt(example):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": example[\"question\"]},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"compressed_question\"]}, # The expected output\n",
        "    ]\n",
        "    # This creates a single string following the model's chat format\n",
        "    # Requires 'tokenizer' to be loaded already\n",
        "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
        "\n",
        "# Apply the formatting - Corrected function name here\n",
        "sft_dataset_formatted = sft_dataset.map(create_chat_prompt)\n",
        "\n",
        "print(f\"--- Example of formatted SFT data ---\\n{sft_dataset_formatted[0]['text']}\")\n",
        "print(\"\\nDataset Prepared and Formatted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312,
          "referenced_widgets": [
            "27effb77762947f29604d613b1c2ba9c",
            "ba563b93ffe84570bb34bec34ba3d2a7",
            "0f50606f678b444d9759d1610ee507ce",
            "4ca8b6db4cef476b93864c192be5a2bc",
            "40c9274ce27d4bc3bfec770d3cee7ee4",
            "69e8dbb48c794f63a75e2de5fd8f9693",
            "9297a540636540028843d7ad4134d161",
            "5cdcd485b914443ba0f880364864683a",
            "6f3d242f982c4cf5b93afa0a1c61cb39",
            "ef6b600f0d654f58a1af225f4cb8c6f3",
            "408f7124341a40babebac13fd32a984f"
          ]
        },
        "id": "-dr4NavZ43w0",
        "outputId": "53456e9b-bf0e-4a45-f8e8-d66a1ea00b7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Arguments Configured (Matched to Fine_Tuning.ipynb).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/25 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27effb77762947f29604d613b1c2ba9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SFTTrainer Initialized.\n",
            "\n",
            "--- Starting SFT (Warmup) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 25 | Num Epochs = 10 | Total steps = 40\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 119,537,664 of 3,940,617,216 (3.03% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 06:11, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.489700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SFT (Warmup) Complete ---\n"
          ]
        }
      ],
      "source": [
        "## CELL: Configure Training Arguments & SFTTrainer (Matching Fine_Tuning.ipynb)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import torch # Ensure torch is imported\n",
        "\n",
        "# --- Define Training Arguments (Matching Fine_Tuning.ipynb) ---\n",
        "sft_training_args = TrainingArguments(\n",
        "    output_dir=\"outputs_phi3_sft\",         # Changed dir name slightly\n",
        "    per_device_train_batch_size=2,         # Matched\n",
        "    gradient_accumulation_steps=4,         # Matched (Effective batch size 8)\n",
        "    learning_rate=2e-4,                    # Matched\n",
        "    warmup_steps=10,                       # Matched\n",
        "    num_train_epochs=10,                   # Kept 10 for small dataset (25 examples)\n",
        "    logging_steps=25,                      # Matched\n",
        "    optim=\"adamw_8bit\",                    # Matched\n",
        "    weight_decay=0.01,                     # Matched\n",
        "    lr_scheduler_type=\"linear\",            # Matched\n",
        "    seed=3407,                             # Matched\n",
        "    save_strategy=\"epoch\",                 # Matched\n",
        "    save_total_limit=2,                    # Matched\n",
        "    fp16=not torch.cuda.is_bf16_supported(), # Matched (Handles T4 correctly)\n",
        "    bf16=torch.cuda.is_bf16_supported(),   # Matched (Handles T4 correctly)\n",
        "    remove_unused_columns=True,            # Good practice\n",
        "    dataloader_pin_memory=False,           # Matched\n",
        "    report_to=\"none\",                      # Matched\n",
        ")\n",
        "print(\"Training Arguments Configured (Matched to Fine_Tuning.ipynb).\")\n",
        "\n",
        "# --- Initialize SFTTrainer (Matching Fine_Tuning.ipynb) ---\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=model,                           # Correct model (Phi-3 + LoRA)\n",
        "    tokenizer=tokenizer,                   # Correct tokenizer\n",
        "    train_dataset=sft_dataset_formatted,   # Your formatted warmup dataset\n",
        "    dataset_text_field=\"text\",             # Matched\n",
        "    max_seq_length=max_seq_length,         # Using 2048 as set above\n",
        "    dataset_num_proc=2,                    # Matched\n",
        "    args=sft_training_args,                # Use the matched arguments\n",
        "    packing=False,                         # Keep False for this format\n",
        ")\n",
        "print(\"SFTTrainer Initialized.\")\n",
        "\n",
        "# --- Run Training ---\n",
        "print(\"\\n--- Starting SFT (Warmup) ---\")\n",
        "sft_trainer.train()\n",
        "print(\"--- SFT (Warmup) Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evuIlAU8CVaL",
        "outputId": "a56de77b-13f6-4b28-ab3f-07e87bf3abaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing SFT-warmed-up Model for Prompt Compression ---\n",
            "\n",
            "Original Question for Test: A restaurant sold 80 pizzas on Friday. On Saturday, they sold 110 pizzas. On Sunday, they sold 130 pizzas. What is the average number of pizzas sold per day over the weekend?\n",
            "\n",
            "Generated Compressed Question:\n",
            "A restaurant sold 80 pizzas on Friday, 110 on Saturday, and 130 on Sunday. What is the average number of pizzas sold per day over the weekend?<|end|>\n"
          ]
        }
      ],
      "source": [
        "## CELL: Testing the SFT Model\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Enable inference mode for the model (important after training)\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"\\n--- Testing SFT-warmed-up Model for Prompt Compression ---\")\n",
        "test_prompt = \"A restaurant sold 80 pizzas on Friday. On Saturday, they sold 110 pizzas. On Sunday, they sold 130 pizzas. What is the average number of pizzas sold per day over the weekend?\"\n",
        "\n",
        "# Assuming system_prompt was defined earlier\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": test_prompt},\n",
        "]\n",
        "\n",
        "# Use apply_chat_template for Phi-3\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Signal for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "print(f\"\\nOriginal Question for Test: {test_prompt}\\n\")\n",
        "print(\"Generated Compressed Question:\")\n",
        "\n",
        "# Generate the compressed prompt\n",
        "_ = model.generate(\n",
        "    input_ids = inputs,\n",
        "    max_new_tokens = 128, # Max length for compressed output\n",
        "    use_cache = True,\n",
        "    temperature = 0.7,    # You can adjust generation parameters\n",
        "    top_p = 0.9,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        "    pad_token_id=tokenizer.eos_token_id, # Set pad token ID\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhIes2d4b4pm"
      },
      "source": [
        "Load up TinyLlama Evaluator, Similarity and Reasoning Embedder for Reward Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al1OfwJ2pKAf",
        "outputId": "d264bede-a772-4a2d-c578-e23c6cb2f2ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading frozen LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "Frozen LLM loaded.\n",
            "Loading cross encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "cross encoder model loaded.\n",
            "Loading embedder model: all-MiniLM-L6-v2\n",
            "Embedder loaded.\n"
          ]
        }
      ],
      "source": [
        "# 1. Load Frozen LLM (TinyLlama)\n",
        "evaluator_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "print(f\"Loading frozen LLM: {evaluator_name}\")\n",
        "evaluator_tokenizer = AutoTokenizer.from_pretrained(evaluator_name)\n",
        "evaluator_model = AutoModelForCausalLM.from_pretrained(\n",
        "    evaluator_name,\n",
        "    dtype=torch.bfloat16,\n",
        "    device_map=\"cuda:0\" # Assumes you're running on a single GPU\n",
        ")\n",
        "evaluator_model.eval() # Set to evaluation mode\n",
        "print(\"Frozen LLM loaded.\")\n",
        "\n",
        "# 2. Load Cross-Encoder for semantic similarity\n",
        "cross_encoder_model_name = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
        "print(f\"Loading cross encoder model: {cross_encoder_model_name}\")\n",
        "cross_encoder = CrossEncoder(cross_encoder_model_name, device=\"cuda:0\")\n",
        "print(\"cross encoder model loaded.\")\n",
        "\n",
        "# 3. Load Embedder for reasoning consistency\n",
        "embedder_model_name = \"all-MiniLM-L6-v2\"\n",
        "print(f\"Loading embedder model: {embedder_model_name}\")\n",
        "embedder = SentenceTransformer(embedder_model_name, device=\"cuda:0\")\n",
        "print(\"Embedder loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qglh2OxpuQzK"
      },
      "source": [
        "Create the reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvbg_mBInOkS"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    cache\n",
        "except NameError:\n",
        "    print(\"Initializing diskcache...\")\n",
        "    cache = diskcache.Cache('./evaluator_cache_tinyllama')\n",
        "    print(\"Diskcache initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ0TUJsymD-E"
      },
      "outputs": [],
      "source": [
        "@cache.memoize()\n",
        "def get_evaluator_output_with_reasoning(prompt, max_new_tokens=256):\n",
        "    \"\"\"\n",
        "    Gets output from TinyLlama, explicitly asking for step-by-step reasoning.\n",
        "    Returns the full output string including reasoning and final answer.\n",
        "    \"\"\"\n",
        "    reasoning_prompt_template = (\n",
        "        \"<|user|>\\n\"\n",
        "        \"Think step-by-step to solve the following math problem. Show your work.\\n\"\n",
        "        \"Problem: {problem}\\n\\n\"\n",
        "        \"Provide your final answer after ####.\"\n",
        "        \"</s>\\n<|assistant|>\"\n",
        "    )\n",
        "    chat_prompt = reasoning_prompt_template.format(problem=prompt)\n",
        "    inputs = evaluator_tokenizer(chat_prompt, return_tensors=\"pt\").to(evaluator_model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = evaluator_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=evaluator_tokenizer.eos_token_id,\n",
        "        )\n",
        "    response_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "    response_text = evaluator_tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "    return response_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8MPYPvvo1ri",
        "outputId": "93e5f19b-972f-42ff-8b67-9adda3678aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated compute_all_reward_metrics_v4 function defined with new final reward weighting.\n"
          ]
        }
      ],
      "source": [
        "def compute_reward_metrics(\n",
        "    orig_prompt, comp_prompt, evaluator_out_original, evaluator_out_comp, ground_truth_answer\n",
        "    ):\n",
        "    metrics = {}\n",
        "    with torch.no_grad():\n",
        "        # Embeddings for reasoning similarity\n",
        "        reasoning_orig = extract_reasoning(evaluator_out_original)\n",
        "        reasoning_comp = extract_reasoning(evaluator_out_comp)\n",
        "        emb_reason_orig = None\n",
        "        emb_reason_comp = None\n",
        "        if reasoning_orig and reasoning_comp:\n",
        "             emb_reason_orig = embedder.encode([reasoning_orig], normalize_embeddings=True, device=device)\n",
        "             emb_reason_comp = embedder.encode([reasoning_comp], normalize_embeddings=True, device=device)\n",
        "\n",
        "        # Embeddings for optional prompt similarity\n",
        "        emb_orig_prompt = None\n",
        "        emb_comp_prompt = None\n",
        "        if orig_prompt and comp_prompt:\n",
        "             emb_orig_prompt = embedder.encode([orig_prompt], normalize_embeddings=True, device=device)\n",
        "             emb_comp_prompt = embedder.encode([comp_prompt], normalize_embeddings=True, device=device)\n",
        "\n",
        "\n",
        "    # 1. Semantic Similarity (r_sem) - Cross-Encoder\n",
        "    r_sem = 0.0\n",
        "    if orig_prompt and comp_prompt:\n",
        "        try:\n",
        "             cross_enc_score = cross_encoder.predict([(orig_prompt, comp_prompt)])\n",
        "             r_sem = float(cross_enc_score[0])\n",
        "        except Exception as e:\n",
        "             print(f\"CrossEncoder prediction failed: {e}\") # Debugging\n",
        "             r_sem = 0.0\n",
        "    metrics['r_sem'] = numpy.clip(r_sem if not numpy.isnan(r_sem) else 0.0, 0.0, 1.0) # Clip assuming positive score\n",
        "\n",
        "    # (Alternate) Semantic Similarity (r_sem) - Bi-Encoder\n",
        "    #r_sem = 0.0\n",
        "    #if emb_orig_prompt is not None and emb_comp_prompt is not None:\n",
        "    #     sim_matrix_prompt = cosine_similarity(emb_orig_prompt, emb_comp_prompt)\n",
        "    #     r_sem = float(sim_matrix_prompt[0][0])\n",
        "    #metrics['r_sem'] = numpy.clip(r_sem if not numpy.isnan(r_sem) else 0.0, -1.0, 1.0)\n",
        "\n",
        "    # 2. Compression Ratio (r_comp)\n",
        "    orig_len = len(orig_prompt.split())\n",
        "    comp_len = len(comp_prompt.split())\n",
        "    r_comp = 1.0 - (comp_len / max(1, orig_len))\n",
        "    metrics['r_comp'] = max(r_comp, -1.0)\n",
        "\n",
        "    # 3. Correctness Score (r_correct)\n",
        "    parsed_evaluator_answer = extract_hash_answer(evaluator_out_comp)\n",
        "    r_correct = 0.0\n",
        "    if parsed_evaluator_answer is not None and ground_truth_answer is not None:\n",
        "        if abs(parsed_evaluator_answer - ground_truth_answer) < 1e-2:\n",
        "            r_correct = 1.0\n",
        "    metrics['r_correct'] = r_correct\n",
        "\n",
        "    # 4. Reasoning Score (r_reason)\n",
        "    r_reason = 0.0\n",
        "    if emb_reason_orig is not None and emb_reason_comp is not None:\n",
        "        sim_matrix = cosine_similarity(emb_reason_orig, emb_reason_comp)\n",
        "        r_reason = float(sim_matrix[0][0])\n",
        "    metrics['r_reason'] = numpy.clip(r_reason if not numpy.isnan(r_reason) else 0.0, -1.0, 1.0)\n",
        "\n",
        "    # 5. FINAL WEIGHTED REWARD\n",
        "    final_reward = (0.5 * metrics['r_correct'] +    # Weight 0.4 (Correctness)\n",
        "                    0.2 * metrics['r_reason'] +     # Weight 0.2 (CoT)\n",
        "                    0.15 * metrics['r_comp'] +       # Weight 0.2 (Compression)\n",
        "                    0.15 * metrics['r_sem'])         # Weight 0.2 (CrossEncoder Prompt Sim)\n",
        "    metrics['final_reward'] = float(final_reward if not numpy.isnan(final_reward) else 0.0)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "print(\"Updated compute_all_reward_metrics_v4 function defined with new final reward weighting.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMTnpu9bq_gd"
      },
      "outputs": [],
      "source": [
        "def final_weighted_reward_func(prompts, completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Calculates the final weighted reward using metrics.\n",
        "    Averages scores across generations for each prompt in the batch.\n",
        "    \"\"\"\n",
        "    avg_final_rewards = []\n",
        "    batch_size = len(completions)\n",
        "    ground_truth_answers = kwargs.get(\"ground_truth_answer\")\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Extract original prompt string correctly from the list of dicts\n",
        "        original_prompt = prompts[i][-1][\"content\"] if isinstance(prompts[i], list) else prompts[i]\n",
        "        ground_truth_answer = ground_truth_answers[i]\n",
        "        generated_prompts = [comp[\"content\"] for comp in completions[i]]\n",
        "\n",
        "        # Get evaluator output (with reasoning) for the original prompt\n",
        "        evaluator_out_original = get_evaluator_output_with_reasoning(original_prompt)\n",
        "\n",
        "        generation_rewards = []\n",
        "        for comp_prompt in generated_prompts:\n",
        "            # Get evaluator output (with reasoning) for the compressed prompt\n",
        "            evaluator_out_comp = get_evaluator_output_with_reasoning(comp_prompt)\n",
        "\n",
        "            # Compute metrics\n",
        "            metrics = compute_reward_metrics(\n",
        "                original_prompt, comp_prompt, evaluator_out_original, evaluator_out_comp, ground_truth_answer\n",
        "            )\n",
        "            generation_rewards.append(metrics['final_reward'])\n",
        "\n",
        "        # Average the final weighted rewards for this prompt, handle empty list\n",
        "        avg_reward = float(numpy.mean(generation_rewards)) if generation_rewards else 0.0\n",
        "        avg_final_rewards.append(avg_reward)\n",
        "\n",
        "    return avg_final_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akNoJ7BUySZf"
      },
      "source": [
        "Set up GRPO Trainer and configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptqkXK2D4d6p",
        "outputId": "824e0569-bc37-4ca8-d71a-b4ef3bce17be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
            "We will change the batch size of 1 to the `num_generations` of 4\n"
          ]
        }
      ],
      "source": [
        "max_prompt_length = 512  # Max length of original prompt + system prompt\n",
        "max_completion_length = 256 # Max length of the *compressed* prompt\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_torch_fused\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    num_generations = 4,\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_completion_length,\n",
        "    #num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 25, # Keep this low for testing\n",
        "    save_steps = 50,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Stays as \"none\"\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "vzOuSVCL_GA9",
        "outputId": "a5dfa7ba-a57f-4f64-92a1-adb43ec0ef15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 2,491 | Num Epochs = 1 | Total steps = 120\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 119,537,664 of 3,940,617,216 (3.03% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  2/120 : < :, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completions / mean_length</th>\n",
              "      <th>completions / min_length</th>\n",
              "      <th>completions / max_length</th>\n",
              "      <th>completions / clipped_ratio</th>\n",
              "      <th>completions / mean_terminated_length</th>\n",
              "      <th>completions / min_terminated_length</th>\n",
              "      <th>completions / max_terminated_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / final_weighted_reward_func / mean</th>\n",
              "      <th>rewards / final_weighted_reward_func / std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 50.12 MiB is free. Process 2424 has 14.69 GiB memory in use. Of the allocated memory 13.64 GiB is allocated by PyTorch, and 917.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3335774225.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothGRPOTrainer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Return inference mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_inference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2329\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/extras/profiling.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mprofiling_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothGRPOTrainer.py\u001b[0m in \u001b[0;36m_prepare_inputs\u001b[0;34m(self, generation_batch)\u001b[0m\n\u001b[1;32m   2008\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgenerate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffered_inputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m                 \u001b[0;31m# self._buffered_inputs=None can occur when resuming from a checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m                 \u001b[0mgeneration_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_and_score_completions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m                 \u001b[0mgeneration_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_pixel_values_by_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothGRPOTrainer.py\u001b[0m in \u001b[0;36m_generate_and_score_completions\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2319\u001b[0m             ):\n\u001b[1;32m   2320\u001b[0m                 \u001b[0mprompt_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2321\u001b[0;31m                 prompt_completion_ids = unwrapped_model.generate(\n\u001b[0m\u001b[1;32m   2322\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mprompt_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/rl.py\u001b[0m in \u001b[0;36mgenerate_with_clone\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0moriginal_generate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrapped_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_with_clone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1973\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36munsloth_fast_generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1772\u001b[0m     \u001b[0;31m# Mixed precision autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEVICE_TYPE_TORCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1774\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1775\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2537\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAMPLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m             \u001b[0;31m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2540\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2868\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2869\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2870\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2872\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/mistral.py\u001b[0m in \u001b[0;36mMistralForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpast_key_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         outputs = LlamaModel_fast_forward_inference(\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mLlamaModel_fast_forward_inference_custom\u001b[0;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m   1071\u001b[0m                 \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             )\n\u001b[0;32m-> 1073\u001b[0;31m             X, present_key_value = attention_fast_forward_inference(\n\u001b[0m\u001b[1;32m   1074\u001b[0m                 \u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mLlamaAttention_fast_forward_inference\u001b[0;34m(self, hidden_states, past_key_value, position_ids, do_prefill, attention_mask)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdo_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaged_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKV_CACHE_INCREMENT\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_kv_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaged_attention_K\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaged_attention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaged_attention_V\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaged_attention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 50.12 MiB is free. Process 2424 has 14.69 GiB memory in use. Of the allocated memory 13.64 GiB is allocated by PyTorch, and 917.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [final_weighted_reward_func],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWzsiZsH0xEU"
      },
      "source": [
        "Test the model with a sample input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcz_lpbVC92"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": (\n",
        "        \"If a shirt costs $45 and is on sale for 30% off, how much does it cost after the discount?\"\n",
        "    )},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    tokenize = False,\n",
        ")\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 512, # Should match your max_completion_length\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "Save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9goBJJlAWyXE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Access the log history from the trainer's state\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Convert the log history (list of dictionaries) into a Pandas DataFrame\n",
        "# Exclude the final entry which might just contain runtime stats\n",
        "metrics_df = pd.DataFrame(log_history[:-1])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(\"--- Training Metrics History ---\")\n",
        "print(metrics_df.to_string()) # .to_string() helps display all rows/columns\n",
        "\n",
        "# You can also access the final summary metrics from the train() output if you captured it\n",
        "# train_result = trainer.train()\n",
        "# print(\"\\n--- Final Training Summary ---\")\n",
        "# print(train_result.metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEAW454BXHvO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# --- 1. Save Adapters Locally in Colab ---\n",
        "output_directory = \"grpo_phi3_adapters\" # Choose a directory name\n",
        "\n",
        "print(f\"Saving model adapters and tokenizer to ./{output_directory}...\")\n",
        "model.save_pretrained(output_directory)\n",
        "tokenizer.save_pretrained(output_directory)\n",
        "print(\"Saving complete.\")\n",
        "\n",
        "# --- 2. Zip the Saved Directory ---\n",
        "zip_filename = f\"{output_directory}.zip\"\n",
        "print(f\"Zipping the directory into {zip_filename}...\")\n",
        "shutil.make_archive(output_directory, 'zip', output_directory)\n",
        "print(\"Zipping complete.\")\n",
        "\n",
        "# --- 3. Download the Zip File ---\n",
        "print(f\"Starting download of {zip_filename}. Please wait...\")\n",
        "files.download(zip_filename)\n",
        "print(\"Download initiated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"gemma-3\")  # Local saving\n",
        "tokenizer.save_pretrained(\"gemma-3\")\n",
        "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "unsloth_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c8f89c484c414a53a017674039994f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f490d1fac94445c88d4b3188f7f45f68",
              "IPY_MODEL_0e5697aa37604c5194fcb2c63cf50679",
              "IPY_MODEL_eda54a9c55894dea9a658cdcbdd420e4"
            ],
            "layout": "IPY_MODEL_ed7d987055b7403db8475d028d45d182"
          }
        },
        "f490d1fac94445c88d4b3188f7f45f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43bcab877623412dad9d507ef89c73bb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2def6f8fa9b6462ea7ba99ffed770ff3",
            "value": "Map:â€‡100%"
          }
        },
        "0e5697aa37604c5194fcb2c63cf50679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ea37091e24143d3b88bd855ad931d78",
            "max": 2491,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87a28938bf634986ad31a666111ca757",
            "value": 2491
          }
        },
        "eda54a9c55894dea9a658cdcbdd420e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b32d051c6073485790e7a1a15fc7e23a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_913284b9fdc140ff80fe511d29bdea4a",
            "value": "â€‡2491/2491â€‡[00:00&lt;00:00,â€‡7661.54â€‡examples/s]"
          }
        },
        "ed7d987055b7403db8475d028d45d182": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43bcab877623412dad9d507ef89c73bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2def6f8fa9b6462ea7ba99ffed770ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ea37091e24143d3b88bd855ad931d78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87a28938bf634986ad31a666111ca757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b32d051c6073485790e7a1a15fc7e23a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "913284b9fdc140ff80fe511d29bdea4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de3c926e20484accb0d6faf1dd595f62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5695906539b74468b65d05a6fa3dfd0f",
              "IPY_MODEL_202724a54bb84aa2882f370a07de7a26",
              "IPY_MODEL_b9555846f727431db1394071f386f2de"
            ],
            "layout": "IPY_MODEL_fa802c2cd3e644679ba5f3c7a612d06d"
          }
        },
        "5695906539b74468b65d05a6fa3dfd0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d559214231404abdaa5dda044d027040",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_282f973d117f4e7991aaa35bafa63200",
            "value": "Map:â€‡100%"
          }
        },
        "202724a54bb84aa2882f370a07de7a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fce34a05e01f470bb13a03cce007cda3",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_944f7b67bdf84676bad404dadbdfc717",
            "value": 25
          }
        },
        "b9555846f727431db1394071f386f2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e3a4bbcd4ca400db08502efd212f743",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3f2cc85149924278bc4893a524897277",
            "value": "â€‡25/25â€‡[00:00&lt;00:00,â€‡715.16â€‡examples/s]"
          }
        },
        "fa802c2cd3e644679ba5f3c7a612d06d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d559214231404abdaa5dda044d027040": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "282f973d117f4e7991aaa35bafa63200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fce34a05e01f470bb13a03cce007cda3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "944f7b67bdf84676bad404dadbdfc717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e3a4bbcd4ca400db08502efd212f743": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f2cc85149924278bc4893a524897277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27effb77762947f29604d613b1c2ba9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba563b93ffe84570bb34bec34ba3d2a7",
              "IPY_MODEL_0f50606f678b444d9759d1610ee507ce",
              "IPY_MODEL_4ca8b6db4cef476b93864c192be5a2bc"
            ],
            "layout": "IPY_MODEL_40c9274ce27d4bc3bfec770d3cee7ee4"
          }
        },
        "ba563b93ffe84570bb34bec34ba3d2a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69e8dbb48c794f63a75e2de5fd8f9693",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9297a540636540028843d7ad4134d161",
            "value": "Unsloth:â€‡Tokenizingâ€‡[&quot;text&quot;]â€‡(num_proc=6):â€‡100%"
          }
        },
        "0f50606f678b444d9759d1610ee507ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cdcd485b914443ba0f880364864683a",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f3d242f982c4cf5b93afa0a1c61cb39",
            "value": 25
          }
        },
        "4ca8b6db4cef476b93864c192be5a2bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef6b600f0d654f58a1af225f4cb8c6f3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_408f7124341a40babebac13fd32a984f",
            "value": "â€‡25/25â€‡[00:02&lt;00:00,â€‡15.14â€‡examples/s]"
          }
        },
        "40c9274ce27d4bc3bfec770d3cee7ee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69e8dbb48c794f63a75e2de5fd8f9693": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9297a540636540028843d7ad4134d161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cdcd485b914443ba0f880364864683a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3d242f982c4cf5b93afa0a1c61cb39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef6b600f0d654f58a1af225f4cb8c6f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "408f7124341a40babebac13fd32a984f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}